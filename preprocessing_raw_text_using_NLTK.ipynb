{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JBQ-dWH_8wr",
        "outputId": "e5e8473b-559b-404f-fdf0-e0b6e2d8578f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "import os  # Provides functions to interact with the operating system\n",
        "import random  # Used to generate random numbers and control randomization (e.g., shuffling data)\n",
        "import nltk  # Natural Language Toolkit, a library for working with human language data (text)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "random.seed(92)\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = str([\"\"\"In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[8]\n",
        "In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[9] and in the following years he went on to develop Word2vec.\n",
        "In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing.\n",
        "That popularity was due partly to a flurry of results showing that such techniques[10][11] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[12] and parsing.[13][14]\n",
        "This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[15] or protect patient privacy.[16]\"\"\"])"
      ],
      "metadata": {
        "id": "_RHJuCLPB6dD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_list = []\n",
        "sentences = nltk.sent_tokenize(str(corpus))\n",
        "for sentence in sentences:\n",
        "\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    token_list.extend(words)\n",
        "print(token_list)\n",
        "print('\\nLength of word_list:', len(token_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBXoyGZWCSoG",
        "outputId": "b79371ff-d2a3-4e3d-fa6b-4b8a9f6cfc64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', \"'In\", '2003', ',', 'word', 'n-gram', 'model', ',', 'at', 'the', 'time', 'the', 'best', 'statistical', 'algorithm', ',', 'was', 'overperformed', 'by', 'a', 'multi-layer', 'perceptron', '(', 'with', 'a', 'single', 'hidden', 'layer', 'and', 'context', 'length', 'of', 'several', 'words', 'trained', 'on', 'up', 'to', '14', 'million', 'of', 'words', 'with', 'a', 'CPU', 'cluster', 'in', 'language', 'modelling', ')', 'by', 'Yoshua', 'Bengio', 'with', 'co-authors', '.', '[', '8', ']', '\\\\nIn', '2010', ',', 'Tomáš', 'Mikolov', '(', 'then', 'a', 'PhD', 'student', 'at', 'Brno', 'University', 'of', 'Technology', ')', 'with', 'co-authors', 'applied', 'a', 'simple', 'recurrent', 'neural', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'to', 'language', 'modelling', ',', '[', '9', ']', 'and', 'in', 'the', 'following', 'years', 'he', 'went', 'on', 'to', 'develop', 'Word2vec.\\\\nIn', 'the', '2010s', ',', 'representation', 'learning', 'and', 'deep', 'neural', 'network-style', '(', 'featuring', 'many', 'hidden', 'layers', ')', 'machine', 'learning', 'methods', 'became', 'widespread', 'in', 'natural', 'language', 'processing.\\\\nThat', 'popularity', 'was', 'due', 'partly', 'to', 'a', 'flurry', 'of', 'results', 'showing', 'that', 'such', 'techniques', '[', '10', ']', '[', '11', ']', 'can', 'achieve', 'state-of-the-art', 'results', 'in', 'many', 'natural', 'language', 'tasks', ',', 'e.g.', ',', 'in', 'language', 'modeling', '[', '12', ']', 'and', 'parsing', '.', '[', '13', ']', '[', '14', ']', '\\\\nThis', 'is', 'increasingly', 'important', 'in', 'medicine', 'and', 'healthcare', ',', 'where', 'NLP', 'helps', 'analyze', 'notes', 'and', 'text', 'in', 'electronic', 'health', 'records', 'that', 'would', 'otherwise', 'be', 'inaccessible', 'for', 'study', 'when', 'seeking', 'to', 'improve', 'care', '[', '15', ']', 'or', 'protect', 'patient', 'privacy', '.', '[', '16', ']', \"'\", ']']\n",
            "\n",
            "Length of word_list: 221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = corpus.split()\n",
        "print(word_list)\n",
        "print('\\nLength of word_list:', len(word_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iWRKWT3CmtE",
        "outputId": "0550dc25-57c2-4401-d786-a5c2f84cad17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"['In\", '2003,', 'word', 'n-gram', 'model,', 'at', 'the', 'time', 'the', 'best', 'statistical', 'algorithm,', 'was', 'overperformed', 'by', 'a', 'multi-layer', 'perceptron', '(with', 'a', 'single', 'hidden', 'layer', 'and', 'context', 'length', 'of', 'several', 'words', 'trained', 'on', 'up', 'to', '14', 'million', 'of', 'words', 'with', 'a', 'CPU', 'cluster', 'in', 'language', 'modelling)', 'by', 'Yoshua', 'Bengio', 'with', 'co-authors.[8]\\\\nIn', '2010,', 'Tomáš', 'Mikolov', '(then', 'a', 'PhD', 'student', 'at', 'Brno', 'University', 'of', 'Technology)', 'with', 'co-authors', 'applied', 'a', 'simple', 'recurrent', 'neural', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'to', 'language', 'modelling,[9]', 'and', 'in', 'the', 'following', 'years', 'he', 'went', 'on', 'to', 'develop', 'Word2vec.\\\\nIn', 'the', '2010s,', 'representation', 'learning', 'and', 'deep', 'neural', 'network-style', '(featuring', 'many', 'hidden', 'layers)', 'machine', 'learning', 'methods', 'became', 'widespread', 'in', 'natural', 'language', 'processing.\\\\nThat', 'popularity', 'was', 'due', 'partly', 'to', 'a', 'flurry', 'of', 'results', 'showing', 'that', 'such', 'techniques[10][11]', 'can', 'achieve', 'state-of-the-art', 'results', 'in', 'many', 'natural', 'language', 'tasks,', 'e.g.,', 'in', 'language', 'modeling[12]', 'and', 'parsing.[13][14]\\\\nThis', 'is', 'increasingly', 'important', 'in', 'medicine', 'and', 'healthcare,', 'where', 'NLP', 'helps', 'analyze', 'notes', 'and', 'text', 'in', 'electronic', 'health', 'records', 'that', 'would', 'otherwise', 'be', 'inaccessible', 'for', 'study', 'when', 'seeking', 'to', 'improve', 'care[15]', 'or', 'protect', 'patient', \"privacy.[16]']\"]\n",
            "\n",
            "Length of word_list: 171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabets_only = [word for word in word_list if word.isalpha()]\n",
        "print(alphabets_only)\n",
        "print('\\nLength of word_list:', len(alphabets_only))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU83VNHuCurP",
        "outputId": "03d38639-d245-47fc-98fe-a2c92e8a1846"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['word', 'at', 'the', 'time', 'the', 'best', 'statistical', 'was', 'overperformed', 'by', 'a', 'perceptron', 'a', 'single', 'hidden', 'layer', 'and', 'context', 'length', 'of', 'several', 'words', 'trained', 'on', 'up', 'to', 'million', 'of', 'words', 'with', 'a', 'CPU', 'cluster', 'in', 'language', 'by', 'Yoshua', 'Bengio', 'with', 'Tomáš', 'Mikolov', 'a', 'PhD', 'student', 'at', 'Brno', 'University', 'of', 'with', 'applied', 'a', 'simple', 'recurrent', 'neural', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'to', 'language', 'and', 'in', 'the', 'following', 'years', 'he', 'went', 'on', 'to', 'develop', 'the', 'representation', 'learning', 'and', 'deep', 'neural', 'many', 'hidden', 'machine', 'learning', 'methods', 'became', 'widespread', 'in', 'natural', 'language', 'popularity', 'was', 'due', 'partly', 'to', 'a', 'flurry', 'of', 'results', 'showing', 'that', 'such', 'can', 'achieve', 'results', 'in', 'many', 'natural', 'language', 'in', 'language', 'and', 'is', 'increasingly', 'important', 'in', 'medicine', 'and', 'where', 'NLP', 'helps', 'analyze', 'notes', 'and', 'text', 'in', 'electronic', 'health', 'records', 'that', 'would', 'otherwise', 'be', 'inaccessible', 'for', 'study', 'when', 'seeking', 'to', 'improve', 'or', 'protect', 'patient']\n",
            "\n",
            "Length of word_list: 141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_case = [word.lower() for word in alphabets_only]\n",
        "print(lower_case)\n",
        "print ('\\nLength of word_list:', len(lower_case))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTM2Pq5oCxUZ",
        "outputId": "a0924518-1385-412e-ce04-e5f993c3c247"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['word', 'at', 'the', 'time', 'the', 'best', 'statistical', 'was', 'overperformed', 'by', 'a', 'perceptron', 'a', 'single', 'hidden', 'layer', 'and', 'context', 'length', 'of', 'several', 'words', 'trained', 'on', 'up', 'to', 'million', 'of', 'words', 'with', 'a', 'cpu', 'cluster', 'in', 'language', 'by', 'yoshua', 'bengio', 'with', 'tomáš', 'mikolov', 'a', 'phd', 'student', 'at', 'brno', 'university', 'of', 'with', 'applied', 'a', 'simple', 'recurrent', 'neural', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'to', 'language', 'and', 'in', 'the', 'following', 'years', 'he', 'went', 'on', 'to', 'develop', 'the', 'representation', 'learning', 'and', 'deep', 'neural', 'many', 'hidden', 'machine', 'learning', 'methods', 'became', 'widespread', 'in', 'natural', 'language', 'popularity', 'was', 'due', 'partly', 'to', 'a', 'flurry', 'of', 'results', 'showing', 'that', 'such', 'can', 'achieve', 'results', 'in', 'many', 'natural', 'language', 'in', 'language', 'and', 'is', 'increasingly', 'important', 'in', 'medicine', 'and', 'where', 'nlp', 'helps', 'analyze', 'notes', 'and', 'text', 'in', 'electronic', 'health', 'records', 'that', 'would', 'otherwise', 'be', 'inaccessible', 'for', 'study', 'when', 'seeking', 'to', 'improve', 'or', 'protect', 'patient']\n",
            "\n",
            "Length of word_list: 141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_nltk = set(stopwords.words('english'))\n",
        "print(stopwords_nltk)\n",
        "print ('\\nLength of word_list:', len(stopwords_nltk), '\\n')\n",
        "\n",
        "cleaned_words = [word for word in lower_case if word not in stopwords_nltk]\n",
        "print(cleaned_words)\n",
        "print ('\\nLength of word_list:', len(cleaned_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOwqZ1jxCxXB",
        "outputId": "1cd34493-5a39-4eeb-ba4a-3bc642e7f0a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'should', \"shouldn't\", 'we', 'don', 'this', 'our', 'against', \"doesn't\", 'ma', 'is', 'any', 't', 'am', 'now', 'its', 'mightn', 'more', 're', 'what', 'have', \"you'll\", 'ours', 'here', 'few', 'm', 'no', \"aren't\", 'each', \"isn't\", 'won', 'most', 'an', \"needn't\", 'yourselves', 'about', 'for', 'once', 'shan', \"you'd\", 'he', 'then', \"haven't\", 'a', 'because', 'i', 'such', 'how', 'further', 'my', 'after', 'herself', 'yourself', 'can', 'ourselves', 'other', 'his', \"hasn't\", 'having', 'as', 'did', 'until', 'you', 'out', \"mightn't\", 'they', 'or', 'me', 'who', 'in', \"that'll\", 'has', 'by', \"couldn't\", \"it's\", 'weren', 'she', 'just', 'at', 'doing', 'again', 'over', 'shouldn', 'through', 'during', 'mustn', \"mustn't\", 'your', 'isn', 'couldn', 'haven', 'when', 'it', 'between', 'aren', 'o', 've', 'while', 'hers', 'down', 'are', 'y', \"wasn't\", \"weren't\", 'off', 'so', 'whom', 'theirs', 'had', 'her', 'd', 'only', 'into', \"didn't\", 'not', 'same', 'above', 'hasn', 'these', 'being', 'ain', 'been', \"should've\", \"you've\", 'needn', 'some', 'if', 'on', 'the', 'does', 'and', 'yours', \"she's\", 'be', 'll', 'wasn', 'myself', 'before', 'there', 'themselves', \"hadn't\", 'was', 'didn', 'from', 'all', 'own', 'doesn', 'him', \"you're\", 'but', 'very', 'too', 'nor', \"don't\", 'itself', 'those', \"won't\", 'why', 'their', 'wouldn', 'that', 'hadn', 'than', 'them', 's', 'do', 'under', 'to', 'will', 'which', 'below', 'both', 'of', 'up', \"wouldn't\", \"shan't\", 'with', 'where', 'were', 'himself'}\n",
            "\n",
            "Length of word_list: 179 \n",
            "\n",
            "['word', 'time', 'best', 'statistical', 'overperformed', 'perceptron', 'single', 'hidden', 'layer', 'context', 'length', 'several', 'words', 'trained', 'million', 'words', 'cpu', 'cluster', 'language', 'yoshua', 'bengio', 'tomáš', 'mikolov', 'phd', 'student', 'brno', 'university', 'applied', 'simple', 'recurrent', 'neural', 'network', 'single', 'hidden', 'layer', 'language', 'following', 'years', 'went', 'develop', 'representation', 'learning', 'deep', 'neural', 'many', 'hidden', 'machine', 'learning', 'methods', 'became', 'widespread', 'natural', 'language', 'popularity', 'due', 'partly', 'flurry', 'results', 'showing', 'achieve', 'results', 'many', 'natural', 'language', 'language', 'increasingly', 'important', 'medicine', 'nlp', 'helps', 'analyze', 'notes', 'text', 'electronic', 'health', 'records', 'would', 'otherwise', 'inaccessible', 'study', 'seeking', 'improve', 'protect', 'patient']\n",
            "\n",
            "Length of word_list: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wz-iW6umCxcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3lI61PHCxe0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}